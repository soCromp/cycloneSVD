{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e96c5d5",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/diffusers/main/en/tutorials/basic_training#train-the-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "170888ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonia/miniconda3/envs/svd/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import diffusers \n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from diffusers.utils import make_image_grid\n",
    "from diffusers import DDPMPipeline\n",
    "import numpy as np \n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "from dataclasses import dataclass\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7562204",
   "metadata": {},
   "source": [
    "# Image model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02218322",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 32  # the generated image resolution\n",
    "    train_batch_size = 16\n",
    "    eval_batch_size = 16  # how many images to sample during evaluation\n",
    "    num_epochs = 500\n",
    "    gradient_accumulation_steps = 8\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    save_image_epochs = 1\n",
    "    save_model_epochs = 1\n",
    "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir = \"debug\"  # the model name locally and on the HF Hub\n",
    "    seed = 0\n",
    "\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dba9aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = diffusers.UNet2DModel(\n",
    "    sample_size        = 32,      # 32×32 tiles\n",
    "    in_channels        = 1,       # wind magnitude only\n",
    "    out_channels       = 1,\n",
    "    block_out_channels = (32, 64, 128),   # 3 resolution scales: 32→16→8\n",
    "    layers_per_block   = 2,\n",
    "    down_block_types   = (\"DownBlock2D\",\n",
    "                          \"AttnDownBlock2D\",\n",
    "                          \"AttnDownBlock2D\"),\n",
    "    up_block_types     = (\"AttnUpBlock2D\",\n",
    "                          \"AttnUpBlock2D\",\n",
    "                          \"UpBlock2D\"),\n",
    "    # cross_attention_dim= 0 # no text conditioning\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "925c43fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyDataset(Dataset):\n",
    "    def __init__(self, dataset, \n",
    "                 width=32, height=32, channels=3, sample_frames=8):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_samples (int): Number of samples in the dataset.\n",
    "            channels (int): Number of channels, default is 3 for RGB.\n",
    "        \"\"\"\n",
    "        # Define the path to the folder containing video frames\n",
    "        self.base_folder = dataset\n",
    "        self.folders = [f for f in os.listdir(self.base_folder) if os.path.isdir(os.path.join(self.base_folder, f))]\n",
    "        self.num_samples = len(self.folders)\n",
    "        self.channels = channels\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.sample_frames = sample_frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sample_frames * len(self.folders)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index of the sample to return.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the 'pixel_values' tensor of shape (16, channels, 320, 512).\n",
    "        \"\"\"\n",
    "        # Randomly select a folder (representing a video) from the base folder\n",
    "        folder_idx = idx // self.sample_frames\n",
    "        frame_idx = idx % self.sample_frames\n",
    "        frame_path = os.path.join(self.base_folder, self.folders[folder_idx], f'{frame_idx}.npy')\n",
    "        \n",
    "        # Initialize a tensor to store the pixel values (3 channels is baked into model)\n",
    "        # pixel_values = torch.empty((self.sample_frames, 3, self.height, self.width))\n",
    "\n",
    "        with Image.fromarray(np.load(frame_path)) as img:\n",
    "            # Resize the image and convert it to a tensor\n",
    "            img_resized = img.resize((self.width, self.height))\n",
    "            img_tensor = torch.from_numpy(np.array(img_resized)).float()\n",
    "            img_tensor[img_tensor.isnan()] = 0.0\n",
    "            if img_tensor.isnan().sum()>0:\n",
    "                raise ValueError(\n",
    "                    f\"{img_tensor.isnan().sum()} NaN values found in the image tensor for frame {frame_name} in folder {chosen_folder}.\")\n",
    "            elif img_tensor.isinf().sum()>0:\n",
    "                raise ValueError(\n",
    "                    f\"Inf values found in the image tensor for frame {frame_name} in folder {chosen_folder}.\")\n",
    "\n",
    "            # Normalize the image by scaling pixel values to [-1, 1]\n",
    "            img_normalized = img_tensor / 255\n",
    "\n",
    "        return {'pixel_values': img_normalized.unsqueeze(0)}\n",
    "    \n",
    "dataset = DummyDataset(dataset='/home/cyclone/train/windmag_atlanticpacific', channels=1)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fa04c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(unet.parameters(), lr=config.learning_rate)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(len(dataloader) * config.num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "922989ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(config, epoch, pipeline):\n",
    "    # Sample some images from random noise (this is the backward diffusion process).\n",
    "    # The default pipeline output type is `List[PIL.Image]`\n",
    "    images = pipeline(\n",
    "        batch_size=config.eval_batch_size,\n",
    "        generator=torch.Generator(device='cpu').manual_seed(config.seed), \n",
    "        # Use a separate torch generator to avoid rewinding the random state of the main training loop\n",
    "    ).images\n",
    "    \n",
    "    images = [Image.fromarray(255*np.array(image)) for image in images]\n",
    "\n",
    "    # Make a grid out of the images\n",
    "    image_grid = make_image_grid(images, rows=4, cols=4)\n",
    "\n",
    "    # Save the images\n",
    "    test_dir = os.path.join(config.output_dir, \"samples\")\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d735139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        log_with=\"tensorboard\",\n",
    "        project_dir=os.path.join(config.output_dir, \"logs\"),\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        if config.output_dir is not None:\n",
    "            os.makedirs(config.output_dir, exist_ok=True)\n",
    "        accelerator.init_trackers(\"train_example\")\n",
    "        \n",
    "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "    \n",
    "    global_step = 0\n",
    "    for epoch in range(config.num_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            clean_images = batch[\"pixel_values\"]\n",
    "            noise = torch.randn(clean_images.shape, device=clean_images.device)\n",
    "            bs = clean_images.shape[0]\n",
    "            \n",
    "            timesteps = torch.randint(\n",
    "                0, noise_scheduler.config.num_train_timesteps, (bs,), device=clean_images.device,\n",
    "                dtype=torch.int64\n",
    "            )\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "            \n",
    "            with accelerator.accumulate(model):\n",
    "                # Predict the noise residual\n",
    "                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "            with open(os.path.join(config.output_dir, 'train_log.txt'), 'a') as f:\n",
    "                f.write(f\"Epoch {epoch}, Step {step}, Loss: {logs['loss']}, LR: {logs['lr']}\\n\")\n",
    "            global_step += 1\n",
    "        \n",
    "        # sample demo images, save model\n",
    "        if accelerator.is_main_process:\n",
    "            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n",
    "            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                evaluate(config, epoch, pipeline)\n",
    "            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                pipeline.save_pretrained(config.output_dir)\n",
    "            \n",
    "noise_scheduler = diffusers.DDPMScheduler(num_train_timesteps=1000)\n",
    "train_loop(config, unet, noise_scheduler, optimizer, dataloader, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53c4e268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x720592891d10>]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "with open('/home/sonia/cycloneSVD/debug/train_log.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "loss_by_epoch = {} \n",
    "for l in lines:\n",
    "    words = l.split()\n",
    "    epoch = int(words[1][:-1])\n",
    "    loss = float(words[5][:-1])\n",
    "    loss_by_epoch[epoch] = loss_by_epoch.get(epoch, []) + [loss]\n",
    "    \n",
    "losses = [] \n",
    "for epoch in sorted(loss_by_epoch.keys()):\n",
    "    losses.append(np.mean(loss_by_epoch[epoch]))\n",
    "    \n",
    "plt.plot(losses)\n",
    "# plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076ca290",
   "metadata": {},
   "source": [
    "# Video model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f090234d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'image_size': 32,  # the generated image resolution\n",
    "    'train_batch_size': 1,\n",
    "    'eval_batch_size': 1,  # how many images to sample during evaluation\n",
    "    'num_epochs': 2,\n",
    "    'gradient_accumulation_steps': 8,\n",
    "    'learning_rate': 1e-7,\n",
    "    'lr_warmup_steps': 0,\n",
    "    'save_image_epochs': 1,\n",
    "    'save_model_epochs': 1,\n",
    "    'output_dir': \"debug\",  # the model name locally and on the HF Hub\n",
    "    'seed': 0,\n",
    "    'dataset': '/home/cyclone/train/windmag_atlanticpacific',\n",
    "    'channels': 1, # channels in the images\n",
    "    'frames': 2,\n",
    "    'continue': False,\n",
    "    'cross_attention_dim': 768,\n",
    "}\n",
    "dtype = torch.float32 # torch.float16 or torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d8ad62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DummyDataset(Dataset):\n",
    "    def __init__(self, dataset, \n",
    "                 width=1024, height=576, channels=3, sample_frames=25):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_samples (int): Number of samples in the dataset.\n",
    "            channels (int): Number of channels, default is 3 for RGB.\n",
    "        \"\"\"\n",
    "        # Define the path to the folder containing video frames\n",
    "        self.base_folder = dataset\n",
    "        self.folders = [f for f in os.listdir(self.base_folder) if os.path.isdir(os.path.join(self.base_folder, f))]\n",
    "        self.num_samples = len(self.folders)\n",
    "        self.channels = channels\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.sample_frames = sample_frames\n",
    "        \n",
    "        # get min, max values for normalization\n",
    "        self.min = np.inf\n",
    "        self.max = -1 * np.inf\n",
    "        for folder in self.folders:\n",
    "            for i in range(sample_frames):\n",
    "                frame = np.load(os.path.join(self.base_folder, folder, f'{i}.npy'))\n",
    "                self.min = min(self.min, frame.min())\n",
    "                self.max = max(self.max, frame.max())\n",
    "                \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index of the sample to return.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the 'pixel_values' tensor of shape (16, channels, 320, 512).\n",
    "        \"\"\"\n",
    "        # Randomly select a folder (representing a video) from the base folder\n",
    "        chosen_folder = random.choice(self.folders)\n",
    "        folder_path = os.path.join(self.base_folder, chosen_folder)\n",
    "        frames = sorted(os.listdir(folder_path))[:self.sample_frames]\n",
    "\n",
    "        # Initialize a tensor to store the pixel values (3 channels is baked into model)\n",
    "        pixel_values = torch.empty((1, self.sample_frames, self.height, self.width))\n",
    "\n",
    "        # Load and process each frame\n",
    "        for i, frame_name in enumerate(frames):\n",
    "            frame_path = os.path.join(folder_path, frame_name)\n",
    "            # with Image.open(frame_path) as img:\n",
    "            with Image.fromarray(np.load(frame_path)) as img:\n",
    "                # Resize the image and convert it to a tensor\n",
    "                img_resized = img.resize((self.width, self.height))\n",
    "                img_tensor = torch.from_numpy(np.array(img_resized)).float()\n",
    "                img_tensor[img_tensor.isnan()] = 0.0\n",
    "                if img_tensor.isnan().sum()>0:\n",
    "                    raise ValueError(\n",
    "                        f\"{img_tensor.isnan().sum()} NaN values found in the image tensor for frame {frame_name} in folder {chosen_folder}.\")\n",
    "                elif img_tensor.isinf().sum()>0:\n",
    "                    raise ValueError(\n",
    "                        f\"Inf values found in the image tensor for frame {frame_name} in folder {chosen_folder}.\")\n",
    "\n",
    "                # Normalize the image by scaling pixel values to [-1, 1]\n",
    "                img_normalized = 2 * (img_tensor - self.min) / (self.max - self.min) - 1\n",
    "\n",
    "                pixel_values[:, i, :, :] = img_normalized\n",
    "        return {'pixel_values': pixel_values}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4d33bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet2d = diffusers.UNet2DConditionModel.from_pretrained(\n",
    "        '/home/sonia/cycloneSVD/img1e-7', subfolder='unet', revision='main')\n",
    "cfg = dict(unet2d.config)\n",
    "cfg['down_block_types'] = ['3D'.join(name.split('2D')) for name in cfg['down_block_types']]  \n",
    "cfg['up_block_types'] = ['3D'.join(name.split('2D')) for name in cfg['up_block_types']]  \n",
    "cfg['mid_block_type'] = '3D'.join(cfg['mid_block_type'].split('2D'))\n",
    "unet  = diffusers.UNet3DConditionModel.from_config(cfg)\n",
    "\n",
    "sd2 = unet2d.state_dict()\n",
    "sd3 = unet.state_dict()\n",
    "for k, w in sd2.items():\n",
    "    w3d = sd3.get(k, np.asarray([]))\n",
    "    if w.ndim == 4 and w3d.ndim==5:                                 # (O, I, H, W)\n",
    "        w3 = w.unsqueeze(2).repeat(1, 1, config['frames'], 1, 1) # add time dimension\n",
    "        if time_avg:                                # Ho et al., 2022\n",
    "            w3 /= config['frames']\n",
    "        sd3[k] = w3\n",
    "    elif w.ndim > w3d.ndim:\n",
    "        sd3[k] = w.squeeze()\n",
    "    else: # no change\n",
    "        sd3[k] = w\n",
    "unet.load_state_dict(sd3, strict=False)\n",
    "# unet = diffusers.UNet3DConditionModel(\n",
    "#         sample_size        = 32,      # 32×32 tiles\n",
    "#         in_channels        = 1,       # wind magnitude only\n",
    "#         out_channels       = 1,\n",
    "#         # norm_num_groups    = 1,\n",
    "#         block_out_channels = [32, 64, 128],   # 3 resolution scales: 32→16→8\n",
    "#         layers_per_block   = 2,\n",
    "#         down_block_types   = [\"CrossAttnDownBlock3D\", \"CrossAttnDownBlock3D\", \"DownBlock3D\"],\n",
    "#         up_block_types     = [\"UpBlock3D\", \"CrossAttnUpBlock3D\", \"CrossAttnUpBlock3D\"],\n",
    "#         # mid_block_type=\"UNetMidBlock3D\",\n",
    "#         cross_attention_dim= config['cross_attention_dim'],\n",
    "#         attention_head_dim=8 # MATCH TO 2D model\n",
    "#     )\n",
    "unet = unet.to(\"cuda\", \n",
    "               dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42ba63ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler = diffusers.DDPMScheduler()\n",
    "\n",
    "dataset = DummyDataset(dataset=config['dataset'], channels=config['channels'], sample_frames=config['frames'],\n",
    "                       width=config['image_size'], height=config['image_size'])\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    unet.parameters(),\n",
    "    lr=config['learning_rate'],)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config['lr_warmup_steps'],\n",
    "    num_training_steps=(len(dataloader) * config['num_epochs']),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d603d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=config['gradient_accumulation_steps'],\n",
    "        log_with=\"tensorboard\",\n",
    "        project_dir=os.path.join(config['output_dir'], \"logs\"),\n",
    "    )\n",
    "    \n",
    "    if accelerator.is_main_process:\n",
    "        os.makedirs(config['output_dir'], exist_ok=True)\n",
    "        accelerator.init_trackers(\"train_example\")\n",
    "        \n",
    "    with open(os.path.join(config['output_dir'], 'config.txt'), 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "        \n",
    "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "        \n",
    "    global_step = 0\n",
    "    for epoch in range(0, config['num_epochs']):\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "        losses = []\n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            clean_images = torch.as_tensor(batch[\"pixel_values\"], dtype=dtype)\n",
    "            zeros = torch.zeros((config['train_batch_size'], 1, config['cross_attention_dim']), dtype=dtype)\n",
    "            \n",
    "            noise = torch.randn(clean_images.shape, device=clean_images.device, dtype=dtype)\n",
    "            bs = clean_images.shape[0]\n",
    "            timesteps = torch.randint(\n",
    "                0, noise_scheduler.config['num_train_timesteps'], (bs,), device=clean_images.device,\n",
    "                dtype=torch.int64\n",
    "            )\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "            # print(noisy_images.shape)\n",
    "            \n",
    "            batchlosses = []\n",
    "            with accelerator.accumulate(model):\n",
    "                noise_pred = model(noisy_images, timesteps, encoder_hidden_states=zeros.to(clean_images.device), \n",
    "                                   return_dict=False)[0]\n",
    "                loss = F.mse_loss(noise_pred[:,:,1:,:,:], noise[:,:,1:,:,:]) # skip zeroth/prompt frame\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "            losses.append(logs['loss'])\n",
    "            global_step += 1\n",
    "            \n",
    "        loss = np.mean(losses)\n",
    "        with open(os.path.join(config['output_dir'], 'train_log.txt'), 'a') as f:\n",
    "            f.write(f\"Epoch {epoch}, Step {global_step}, Loss: {loss}, LR: {logs['lr']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c85c319",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonia/miniconda3/envs/svd/lib/python3.11/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "Epoch 0:   0%|          | 0/15882 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   3%|▎         | 533/15882 [00:27<13:01, 19.64it/s, loss=0.0893, lr=1e-7, step=532]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mtrain_loop\u001b[39m\u001b[34m(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\u001b[39m\n\u001b[32m     22\u001b[39m progress_bar.set_description(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m losses = []\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclean_images\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpixel_values\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mzeros\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain_batch_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcross_attention_dim\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/svd/lib/python3.11/site-packages/accelerate/data_loader.py:577\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    575\u001b[39m     current_batch = send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m.device, non_blocking=\u001b[38;5;28mself\u001b[39m._non_blocking)\n\u001b[32m    576\u001b[39m \u001b[38;5;28mself\u001b[39m._update_state_dict()\n\u001b[32m--> \u001b[39m\u001b[32m577\u001b[39m next_batch = \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_index >= \u001b[38;5;28mself\u001b[39m.skip_batches:\n\u001b[32m    579\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/svd/lib/python3.11/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/svd/lib/python3.11/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/svd/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/svd/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mDummyDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     40\u001b[39m chosen_folder = random.choice(\u001b[38;5;28mself\u001b[39m.folders)\n\u001b[32m     41\u001b[39m folder_path = os.path.join(\u001b[38;5;28mself\u001b[39m.base_folder, chosen_folder)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m frames = \u001b[38;5;28msorted\u001b[39m(os.listdir(folder_path))[:\u001b[38;5;28mself\u001b[39m.sample_frames]\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Initialize a tensor to store the pixel values (3 channels is baked into model)\u001b[39;00m\n\u001b[32m     45\u001b[39m pixel_values = torch.empty((\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.sample_frames, \u001b[38;5;28mself\u001b[39m.height, \u001b[38;5;28mself\u001b[39m.width))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   3%|▎         | 533/15882 [00:40<13:01, 19.64it/s, loss=0.0893, lr=1e-7, step=532]"
     ]
    }
   ],
   "source": [
    "train_loop(config, unet, noise_scheduler, optimizer, dataloader, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "980e0c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict([('sample_size', 32),\n",
       "            ('in_channels', 1),\n",
       "            ('out_channels', 1),\n",
       "            ('center_input_sample', False),\n",
       "            ('flip_sin_to_cos', True),\n",
       "            ('freq_shift', 0),\n",
       "            ('down_block_types',\n",
       "             ['CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'DownBlock2D']),\n",
       "            ('mid_block_type', 'UNetMidBlock2DCrossAttn'),\n",
       "            ('up_block_types',\n",
       "             ['UpBlock2D', 'CrossAttnUpBlock2D', 'CrossAttnUpBlock2D']),\n",
       "            ('only_cross_attention', False),\n",
       "            ('block_out_channels', [32, 64, 128]),\n",
       "            ('layers_per_block', 2),\n",
       "            ('downsample_padding', 1),\n",
       "            ('mid_block_scale_factor', 1),\n",
       "            ('dropout', 0.0),\n",
       "            ('act_fn', 'silu'),\n",
       "            ('norm_num_groups', 32),\n",
       "            ('norm_eps', 1e-05),\n",
       "            ('cross_attention_dim', 768),\n",
       "            ('transformer_layers_per_block', 1),\n",
       "            ('reverse_transformer_layers_per_block', None),\n",
       "            ('encoder_hid_dim', None),\n",
       "            ('encoder_hid_dim_type', None),\n",
       "            ('attention_head_dim', 8),\n",
       "            ('num_attention_heads', None),\n",
       "            ('dual_cross_attention', False),\n",
       "            ('use_linear_projection', False),\n",
       "            ('class_embed_type', None),\n",
       "            ('addition_embed_type', None),\n",
       "            ('addition_time_embed_dim', None),\n",
       "            ('num_class_embeds', None),\n",
       "            ('upcast_attention', False),\n",
       "            ('resnet_time_scale_shift', 'default'),\n",
       "            ('resnet_skip_time_act', False),\n",
       "            ('resnet_out_scale_factor', 1.0),\n",
       "            ('time_embedding_type', 'positional'),\n",
       "            ('time_embedding_dim', None),\n",
       "            ('time_embedding_act_fn', None),\n",
       "            ('timestep_post_act', None),\n",
       "            ('time_cond_proj_dim', None),\n",
       "            ('conv_in_kernel', 3),\n",
       "            ('conv_out_kernel', 3),\n",
       "            ('projection_class_embeddings_input_dim', None),\n",
       "            ('attention_type', 'default'),\n",
       "            ('class_embeddings_concat', False),\n",
       "            ('mid_block_only_cross_attention', None),\n",
       "            ('cross_attention_norm', None),\n",
       "            ('addition_embed_type_num_heads', 64),\n",
       "            ('_class_name', 'UNet2DConditionModel'),\n",
       "            ('_diffusers_version', '0.35.0.dev0'),\n",
       "            ('_name_or_path', '/home/sonia/cycloneSVD/img1e-7')])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet2d.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7214c226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
